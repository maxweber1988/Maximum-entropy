In this section we describe the numerical algorithm to perform the analytic continuation of quantum Monte Carlo data by the maximum entropy method. As described in Sec. \ref{sec:theory} the quantity $Q = \alpha S - L$ has to be maximized with respect to $\vec A$ in order to find the most probable $\vec A$ given the noisy Greens function $\vec G$. To maximize Q we calculate the gradient of Q with respect to $\vec A$ and set it to zero:
\begin{equation}
	\vec\nabla Q =  \alpha \vec\nabla S - \vec\nabla L = 0
	\label{numerical_algorithm:equ.1}
\end{equation}
Then by making use of Equ \ref{eq:S(A,m)} for $S$ and Equ. \ref{eq:L(G)} for $L$ Equ. \ref{numerical_algorithm:equ.1} leads to:
\begin{equation}
	- \alpha \log \bigg(\frac{A_i}{m_i} \bigg) = \sum K_{ji} \frac{\partial L}{\partial \vec F}
	\label{numerical_algorithm:equ.2}
\end{equation}
where:
\begin{equation}
	\vec F = \mathbf{K} \vec A \text{ and } \vec \nabla L = \frac{\partial \vec F}{\partial \vec A} \frac{\partial L}{\partial \vec F} = \mathbf{K}^T \frac{\partial L}{\partial \vec F}
	\label{numerical_algorithm:equ.3}
\end{equation}
Now, a singular value decomposition of $\mathbf{K}$ is performed with $\mathbf{K} = \mathbf{V} \mathbf{\Sigma} \mathbf{U}^T$. Please note that both $\mathbf{V}$ and $\mathbf{U}$ are orthonormal matrices and therefor $\mathbf{V}^{-1}=\mathbf{V}^T$ and $\mathbf{U}^{-1}=\mathbf{U}^T$. Then Equ. \ref{numerical_algorithm:equ.3} can be rewritten as
\begin{equation}
	-\alpha \log \bigg(\frac{\vec A}{\vec m}\bigg) = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T \frac{\partial L}{\partial \vec F} 
	\label{numerical_algorithm:equ.4}
\end{equation}
where Equ. \ref{numerical_algorithm:equ.4} has to be read component wise.
Here $\mathbf{\Sigma}$ has only components on its diagonal which are called the singular values of $\mathbf{K}$. By convention the singular values are ordered by magnitude. Defining $\vec u = \alpha^{-1} \mathbf{\Sigma} \mathbf{V}^T \frac{\partial L}{\partial \vec F}$ $\vec A$ can be represented by $\vec u$ as
\begin{equation}
	A_i = m_i \exp \Big(\sum U_{in}u_n\Big)
	\label{numerical_algorithm:equ.5}
\end{equation}
Equ. \ref{numerical_algorithm:equ.5} has the advantage that it automatically enforces positivity for $\vec A$.
Now Bryan argues that unless $\mathbf{K}$ is of full rank the components of $\vec u$ will not be independent.
Because of the limited precision of the computer and the singular value decomposition some of the singular values of $\mathbf{K}$ will effectively be zero. The search for the optimal $\vec u$ can therefor be reduced to the nonzero singular values of $\mathbf{K}$. Let $s$ be the number of nonzero singular values the search can then be limited to the $s$-dimensional space which Bryan calls the singular space. 
Bryan's method therefor first reduces all relevant matrices to the singular space. The vector $\vec u$ is now of length $s$, the number of columns of $\mathbf{V}$ and $\mathbf{U}$ are reduced to $s$ and $\mathbf{\Sigma}$ is now a $s \times s$ square matrix. Making use of Equ. \ref{numerical_algorithm:equ.5} and $\mathbf{K} = \mathbf{V} \mathbf{\Sigma} \mathbf{U}^T$ Equ. \ref{numerical_algorithm:equ.2} can be rewritten as:
\begin{equation}
	-\alpha \mathbf{U} \vec u = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T \frac{\partial L}{\partial \vec F}
	\label{numerical_algorithm:equ.6}
\end{equation}
Multiplying Equ. \ref{numerical_algorithm:equ.6} by $\mathbf{U}^T$ on both sides it reduces to
\begin{equation}
	-\alpha \vec u = \mathbf{\Sigma} \mathbf{V}^T \frac{\partial L}{\partial \vec F} \equiv \vec g 
	\label{numerical_algorithm:equ.7}
\end{equation}
or
\begin{equation}
	-\alpha \vec u - \vec g = 0
	\label{numerical_algorithm:equ.8}
\end{equation}
Equ. \ref{numerical_algorithm:equ.8} can be solved by a multidimensional Newton search iteratively
\begin{equation}
	\mathbf{J} \vec{\delta u} = -\alpha \vec u - \vec g
	\label{numerical_algorithm:equ.9}
\end{equation}
where $\mathbf{J} = \alpha \mathbf{I} + \frac{\partial \vec g}{\partial \vec u}$ is the Jacobian and $\mathbf{I}$ the identity matrix. 
With $\mathbf{W} = \frac{\partial^2 L}{\partial^2 \vec F}$, $\mathbf{M} = \mathbf{\Sigma}\mathbf{V}^T\mathbf{W}\mathbf{V}\mathbf{\Sigma}$ and $\mathbf{T} = \mathbf{U}^T \vec A \mathbf{U}$ Equ. \ref{numerical_algorithm:equ.9} reads
\begin{equation}
	((\alpha + \mu) \mathbf{I} + \mathbf{M}\mathbf{T}) \vec{\delta u} = -\alpha \vec u - \vec g
	\label{numerical_algorithm:equ.10}
\end{equation}
At each iteration step of the Newton search the step length $\vec{\delta u}$ muss be restricted for the stability of the algorithm.
Therefor, a Levenberg-Marquardt parameter $\mu$ is added in Equ. \ref{numerical_algorithm:equ.10} to ensure stability.
Bryan proposes 
\begin{equation}
	\vec{\delta u}^T \frac{\partial \vec A}{\partial \vec u}\mathbf{diag}\{\frac{1}{A_i}\}\frac{\partial \vec A}{\partial \vec u}
	\vec{\delta u}^T = \vec{\delta u}^T \mathbf{T} \vec{\delta u} \leq \sum m_i
\end{equation}
as a maximum step length for the algorithm.\newline
Remark:\newline
\textit{However, we found that sometimes even if this criterion is fulfilled the algorithm can be instable due to numerical overflow. The reason for this we assume to be the invalid linear approximation of $\vec {\delta A} = \mathbf{U}^T \vec A\vec{\delta u}$, i.e. $\vec{\delta A} << \vec A(\vec u + \vec{\delta u}) - \vec A(\vec u)$. 
Therefor, we use the criterion $\parallel \vec A \parallel^2 \leq \sum_i m_i$. In addition we need to choose a reasonable value for the Lewenberg-Marquardt parameter $\mu$. For large values of $\mu$ the Newton search becomes the method of steepest descent. For $\mu >> \alpha: (\mu \mathbf{I} + \mathbf{J})\vec{\delta u} = \nabla Q \rightarrow \mu \mathbf{I} \vec{\delta u} = \nabla Q$. The method for steepest descent reads as $\vec{\delta u} = h \nabla Q$. Therefor we can identify $\mu$ as $h^{-1}$. As the method of steepest descent is a first order approximation algorithm we need to choose $h$ in a way that this approximation stays valid. We therefor demand that the relative error $r$ calculated as the quotient of the second and first order term in a Taylor expansion of $Q$ $r = (h\nabla Q^T \mathbf{J}\nabla Q)^{-1}\nabla Q^T\nabla Q = 0.01$.}\newline
Now the Newton search can be made more efficient by diagonalizing Equ. \ref{numerical_algorithm:equ.10}. First we diagonalize $\mathbf{T}$:
\begin{equation}
	\begin{gathered}
		\mathbf{T} \mathbf{P} = \mathbf{P} \mathbf{\Gamma},\\
		\mathbf{\Gamma} = \mathbf{diag} \{\gamma_i\}
	\end{gathered}
	\label{numerical_algorithm:equ.11}
\end{equation}
Then we define
\begin{equation}
	\mathbf{B} = \mathbf{diag} \{ \gamma_i^{\frac{1}{2}}\}\mathbf{P}^T \mathbf{M}\mathbf{P}\mathbf{diag}\{ \gamma_i^{\frac{1}{2}}\}
	\label{numerical_algorithm:equ.12}
\end{equation}
and again solve the eigenvalue equation
\begin{equation}
	\begin{gathered}
		\mathbf{B} \mathbf{R} = \mathbf{R} \mathbf{\Lambda},\\
		\mathbf{\Lambda} = \mathbf{diag} \{\lambda_i\}
	\end{gathered}
	\label{numerical_algorithm:equ.13}
\end{equation}
Please note that $\mathbf{P}$ and $\mathbf{R}$ are orthogonal matrices and $\gamma_i$ and $\lambda_i$ the eigenvalues of $\mathbf{T}$ and $\mathbf{B}$. Then to diagonalize Equ. \ref{numerical_algorithm:equ.10} we define 
\begin{equation}
	\mathbf{Y} = \mathbf{P} \mathbf{diag}\{ \gamma_i^{-\frac{1}{2}}\} \mathbf{R}
	\label{numerical_algorithm:equ.14}
\end{equation}
With $\mathbf{Y}^{-T}\mathbf{Y}^{-1} = \mathbf{T}$ and $\mathbf{Y}^{-1}\mathbf{M}\mathbf{Y}^{-T} = \mathbf{\Lambda}$ Equ. 
\ref{numerical_algorithm:equ.10}
can be rewritten as
\begin{equation}
	[( \alpha + \mu) \mathbf{I} +\mathbf{\Lambda}]\mathbf{Y}^{-1} \vec{\delta u} = \mathbf{Y}^{-1}[ -\alpha \vec u - \vec g]
	\label{numerical_algorithm:equ.15}
\end{equation}
which leads to s independent equations for $\mathbf{Y}^{-1} \vec{\delta u}$.
Now Equ. \ref{numerical_algorithm:equ.10} can be rewritten to
\begin{equation}
	(\alpha + \mu)\vec{\delta u} = -\alpha \vec u - g - \mathbf{M}\mathbf{Y}^{-T}\mathbf{Y}^{-1} \vec{\delta u}
	\label{numerical_algorithm:equ.16}
\end{equation}
So to finally we first solve Equ. \ref{numerical_algorithm:equ.15} for $\mathbf{Y}^{-1} \vec{\delta u}$, use it in Equ. \ref{numerical_algorithm:equ.16} to solve for $\vec{\delta u}$ and calculate the new value for $\vec u_{n+1} = \vec u_{n} + \vec{\delta u}$. The iteration is terminated if $\sum_i |\vec u_{n+1} - \vec u_{n}| \leq 10^{-10}$. There exists however no deeper motivation for this criterion. We should notice that in practice Equ. \ref{numerical_algorithm:equ.14} can be ill defined due to very small $\gamma_i$ values. But only $\mathbf{Y}^{-1}$ is needed in all the calculations and it is straight forward to find the expression $\mathbf{Y}^{-1} = \mathbf{R}^T \mathbf{diag} \{ \gamma_i^{\frac{1}{2}}\}\mathbf{P}^T$ because of the orthogonality of $\mathbf{P}$ and $\mathbf{R}$.
